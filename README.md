# MLOps Rakuten

Product type classification for Rakuten France

---

## Project Organization

```
‚îú‚îÄ‚îÄ Makefile           <- Makefile with convenience commands like `make data` or `make train`
‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
‚îú‚îÄ‚îÄ data
‚îÇ   ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
‚îÇ   ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
‚îÇ   ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
‚îÇ   ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml    <- Docker containers orchestration
‚îÇ
‚îú‚îÄ‚îÄ docker
‚îÇ   ‚îú‚îÄ‚îÄ api-service
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile      <- Configuration for the Base container
‚îÇ
‚îú‚îÄ‚îÄ deployments
‚îÇ   ‚îú‚îÄ‚îÄ certs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nginx.crt       <- Nginx certificate
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nginx.key       <- Certificate key
‚îÇ   ‚îú‚îÄ‚îÄ nginx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nginx.conf      <- Configuration for Nginx
‚îÇ   ‚îî‚îÄ‚îÄ prometheus
‚îÇ       ‚îî‚îÄ‚îÄ prometheus.yml  <- Configuration for Prometheus
‚îÇ
‚îú‚îÄ‚îÄ docs               <- A default mkdocs project; see www.mkdocs.org for details
‚îÇ
‚îú‚îÄ‚îÄ logs               <- Contains all log and error files
‚îÇ
‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
‚îÇ
‚îú‚îÄ‚îÄ notebooks
‚îÇ   ‚îî‚îÄ‚îÄ 01_exploration.ipynb  <- Text data exploration
‚îÇ
‚îÇ
‚îú‚îÄ‚îÄ pyproject.toml     <- Project configuration file with package metadata for
‚îÇ                         mlops_rakuten and configuration for tools like black
‚îÇ
‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
‚îÇ
‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
‚îÇ   ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
‚îÇ
‚îú‚îÄ‚îÄ requirements-dev.txt   <- The requirements file for development environment
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment
‚îÇ
‚îú‚îÄ‚îÄ tests
‚îÇ   ‚îú‚îÄ‚îÄ test_pipelines.py            <- Test all the pipelines
‚îÇ   ‚îú‚îÄ‚îÄ test_data_ingestion.py       <- Test Data Ingestion
‚îÇ   ‚îú‚îÄ‚îÄ test_data_preprocessing.py   <- Test Data Preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ test_data_transformation.py  <- Test Data Transformation
‚îÇ   ‚îú‚îÄ‚îÄ test_model_trainer.py        <- Test Model Trainer
‚îÇ   ‚îú‚îÄ‚îÄ test_model_evaluation.py     <- Test Model Evaluation
‚îÇ   ‚îî‚îÄ‚îÄ test_prediction.py           <- Test Prediction
‚îÇ
‚îî‚îÄ‚îÄ mlops_rakuten   <- Source code for use in this project.
    ‚îÇ
    ‚îú‚îÄ‚îÄ __init__.py             <- Makes mlops_rakuten a Python module
    ‚îÇ
    ‚îú‚îÄ‚îÄ main.py                 <- Scripts to train model or make prediction
    ‚îÇ
    ‚îú‚îÄ‚îÄ services
    ‚îÇ   ‚îú‚îÄ‚îÄ gateway_app.py          <- API Gateway
    ‚îÇ   ‚îú‚îÄ‚îÄ ingest_app.py           <- API Ingest Service
    ‚îÇ   ‚îú‚îÄ‚îÄ predict_app.py          <- API Predict Service
    ‚îÇ   ‚îú‚îÄ‚îÄ schemas_app.py          <- pydantic Models
    ‚îÇ   ‚îî‚îÄ‚îÄ train_app.py            <- API Train Service
    ‚îÇ
    ‚îú‚îÄ‚îÄ config
    ‚îÇ   ‚îú‚îÄ‚îÄ auth_simple.py          <- OAuth2 authentication
    ‚îÇ   ‚îú‚îÄ‚îÄ hash_password.py        <- Utility script for getting password hash
    ‚îÇ   ‚îî‚îÄ‚îÄ users.json              <- Users and Admins lists
    ‚îÇ
    ‚îú‚îÄ‚îÄ config
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ config_manager.py       <- Create Config objects
    ‚îÇ   ‚îú‚îÄ‚îÄ config.yml              <- Parameters for Config objects
    ‚îÇ   ‚îú‚îÄ‚îÄ constants.py            <- Store useful variables and configuration
    ‚îÇ   ‚îî‚îÄ‚îÄ entities.py             <- Modules used to process data and train model
    ‚îÇ
    ‚îú‚îÄ‚îÄ modules
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ data_seeding.py         <- Code to split initial dataset
    ‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py       <- Code to merge new dataset
    ‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py   <- Code to clean data
    ‚îÇ   ‚îú‚îÄ‚îÄ data_transformation.py  <- Code for TF-IDF and train / test split
    ‚îÇ   ‚îú‚îÄ‚îÄ model_trainer.py        <- Code for Linear SVC
    ‚îÇ   ‚îú‚îÄ‚îÄ model_evaluation.py     <- Code for evaluating Linear SVC performances
    ‚îÇ   ‚îî‚îÄ‚îÄ prediction.py           <- Code for running inference
    ‚îÇ
    ‚îú‚îÄ‚îÄ pipelines
    ‚îÇ   ‚îú‚îÄ‚îÄ data_seeding.py         <- Data seeding pipeline
    ‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py       <- Data ingestion pipeline
    ‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py   <- Data Preprocessing pipeline
    ‚îÇ   ‚îú‚îÄ‚îÄ data_transformation.py  <- Data Transformation pipeline
    ‚îÇ   ‚îú‚îÄ‚îÄ model_trainer.py        <- Model Trainer pipeline
    ‚îÇ   ‚îú‚îÄ‚îÄ model_evaluation.py     <- Model Evaluation pipeline
    ‚îÇ   ‚îî‚îÄ‚îÄ prediction.py           <- Prediction pipeline
    ‚îÇ
    ‚îî‚îÄ‚îÄ utils.py                <- Create directory and read YAML file
```

---

## Installation

### 1. Environnement Python

1. V√©rifier si `uv` est install√©, sinon [Installer uv](https://docs.astral.sh/uv/getting-started/installation/).
   `$ uv --version`

2. Cr√©ation de l'environnement Python (macOs / Linux)
   `$ make create_environment`

3. Activer l'environnement Python (macOS / Linux)
   `$ source .venv/bin/activate`

4. Installer les d√©pendances
   `$ make requirements`

5. V√©rifier que l'environnement est op√©rationnel
   `$ python -c "import pandas, typer, mlops_rakuten; print('OK')"`

### 2. Configuration des donn√©es

Les donn√©es ne sont pas incluses dans le repository. Vous devez les t√©l√©charger manuellement.

#### √âtape 1 : T√©l√©charger les donn√©es

- Acc√©der au dossier partag√©
- T√©l√©charger les fichiers :
  - `X_train_update.csv`
  - `Y_train_CVw08PX.csv`
  - `product_categories.csv`

#### √âtape 2 : Cr√©er le dossier et copier les fichiers
```bash
# Cr√©er le dossier data/raw/rakuten s'il n'existe pas
$ mkdir -p data/raw/rakuten
```

#### √âtape 3 : Copier-Coller les fichiers dans les repertoires
- `product_categories.csv` dans `data/raw/`
- `X_train_update.csv` et `Y_train_CVw08PX.csv` dans `data/raw/rakuten`

#### √âtape 4 : V√©rifier
```bash
# V√©rifier la pr√©sence des fichiers
$ ls data/raw/
$ ls data/raw/rakuten

# Devrait afficher :
# product_categories.csv
# X_train_update.csv
# Y_train_CVw08PX.csv
```

---

## Structure

### Step 1: Classes de Configuration
- mlops_rakuten/config.py d√©finit les variables globales contenant les chemins vers les r√©pertoires et fichiers.
- mlops_rakuten/config.yml d√©finit tous les chemins vers les fichiers qui seront utilis√©s ou cr√©√©s √† chaque √©tape du pipeline.
- mlops_rakuten/entities.py d√©finit toutes les classes qui seront utilis√©s comme configuration.

### Step 2: Configuration Manager
- mlops_rakuten/config_manager.py cr√©e les objets de configuration en s‚Äôappuyant sur les classes d√©finies pr√©alablement.
  + DataSeedingConfig
  + DataIngestionConfig
  + DataPreprocessingConfig
  + DataTransformationConfig
  + ModelTrainerConfig
  + ModelEvaluationConfig

### Step 3: les modules de Data et Model et Predict
- mlops_rakuten/modules/ d√©finit les modules utilis√©s dans les pipelines Data et Model:
  + mlops_rakuten/modules/data_seeding.py d√©finit le module de DataSeeding (d√©coupage des donn√©es initiales)
  + mlops_rakuten/modules/data_ingestion.py d√©finit le module de DataIngestion (fusion des datasets features et target)
  + mlops_rakuten/modules/data_preprocessing.py d√©finit le module de DataPreprocessing (n/a, outliers, duplicates, etc.)
  + mlops_rakuten/modules/data_transformation.py d√©finit le module de DataTransformation (TF-IDF et train / test split, sauvegarde des artifacts)
  + mlops_rakuten/modules/model_trainer.py d√©finit le module de ModelTrainer (Linear SVC, sauvegarde des artifacts)
  + mlops_rakuten/modules/model_evaluation.py d√©finit le module de ModelEvaluation (metrics et matrice de confusion)

### Step 4: √âtapes du Pipeline
- mlops_rakuten/pipelines/ d√©finit les pipelines qui seront instanci√©s et ex√©cut√©s:
  + mlops_rakuten/pipelines/data_seeding.py
  + mlops_rakuten/pipelines/data_ingestion.py
  + mlops_rakuten/pipelines/data_preprocessing.py
  + mlops_rakuten/pipelines/data_transformation.py
  + mlops_rakuten/pipelines/model_trainer.py
  + mlops_rakuten/pipelines/model_evaluation.py

### Step 5: Ex√©cution de la Pipeline compl√®te
- mlops_rakuten/main.py permet d'ex√©cuter l'ensemble de la Pipeline.

---

## Donn√©es requises

### Ex√©cution via Docker

Pour pouvoir entra√Æner un mod√®le, le fichier suivant doit exister **dans le volume Docker** :

* `/app/data/interim/rakuten_train.csv`

Ce fichier est pr√©sent **en local** dans le d√©p√¥t, √† l‚Äôemplacement :

* `data/interim/rakuten_train.csv`

Il n‚Äôest **pas copi√© automatiquement** au d√©marrage des conteneurs.
L‚Äôinjection dans le volume Docker est **volontairement explicite**, afin de rester compatible avec une future int√©gration DVC / Dagshub.

> √Ä terme, cette √©tape sera remplac√©e par un `dvc pull`.

---

## Lancer l‚Äôapplication avec Docker

### 1. D√©marrer la stack compl√®te

```bash
make docker-up
```

V√©rifier que les conteneurs sont bien lanc√©s :

```bash
make docker-ps
```

---

### 2. Injecter le fichier d‚Äôentra√Ænement dans le volume Docker

```bash
make docker-cp-traincsv
```

Cette commande :

* copie `data/interim/rakuten_train.csv` (local)
* vers `/app/data/interim/rakuten_train.csv` dans le volume Docker

üëâ **√âtape obligatoire avant le premier entra√Ænement**.

---

### 3. Acc√©der √† Swagger

```bash
make swagger
```

Puis ouvrir dans le navigateur :

* [https://localhost/docs](https://localhost/docs)

---

## Tester l‚Äôapplication (Swagger)

### 1. Authentification

* Endpoint : `POST /token`
* Fournir un `username` et un `password`
* R√©cup√©rer le `access_token`

Cliquer ensuite sur **Authorize** et renseigner :

```
Bearer <access_token>
```

---

### 2. Entra√Æner un mod√®le

* Endpoint : `POST /train`

Comportement attendu :

* cr√©ation d‚Äôun r√©pertoire `/app/data/processed/<timestamp>/`
* entra√Ænement du mod√®le
* sauvegarde du mod√®le dans :

```
/app/models/<timestamp>/text_classifier.pkl
```

---

### 3. V√©rifier l‚Äô√©tat du mod√®le

* Endpoint : `GET /info`

Retourne notamment :

* si un mod√®le est disponible (`ready`)
* le chemin du mod√®le utilis√©
* le dernier jeu de donn√©es trait√©

---

### 4. Faire une pr√©diction

* Endpoint : `POST /predict`

Payload attendu :

```json
{
  "designation": "Tr√®s joli pull pour enfants",
  "top_k": 3
}
```

---

## Tests en ligne de commande (curl)

> L‚Äôoption `-k` est n√©cessaire en cas de certificat TLS auto-sign√©.

### R√©cup√©rer un token

```bash
curl -k -X POST https://localhost/token \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=julien&password=admin123"
```

---

### Lancer un entra√Ænement

```bash
curl -k -X POST https://localhost/train \
  -H "Authorization: Bearer <TOKEN>"
```

---

### Informations sur le mod√®le

```bash
curl -k https://localhost/info \
  -H "Authorization: Bearer <TOKEN>"
```

---

### Pr√©diction

```bash
curl -k -X POST https://localhost/predict \
  -H "Authorization: Bearer <TOKEN>" \
  -H "Content-Type: application/json" \
  -d '{"designation":"Tr√®s joli pull pour enfants","top_k":3}'
```

---

## Nginx / TLS

Pour g√©n√©rer un certificat auto-sign√© (exemple avec `mkcert`) :

```bash
mkcert -key-file deployments/certs/nginx.key \
      -cert-file deployments/certs/nginx.crt \
      localhost 127.0.0.1 ::1
```

---

## Commandes Makefile (Docker)

Commandes principales :

* `make docker-up`
  Build et d√©marre l‚Äôensemble des services

* `make docker-down`
  Arr√™te les services (volumes conserv√©s)

* `make docker-down-v`
  Arr√™te les services **et supprime les volumes** (‚ö†Ô∏è destructif)

* `make docker-cp-traincsv`
  Injecte `rakuten_train.csv` dans le volume Docker

* `make docker-logs`
  Affiche les logs des conteneurs

* `make swagger`
  Ouvre Swagger dans le navigateur

---

## Mots de passe

* `jane` : `password`
* `john` : `password`
* `julien` : `admin123`
* `claudia` : `admin456`
* `samuel` : `admin789`
